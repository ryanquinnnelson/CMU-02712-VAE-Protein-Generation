{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33830e9c-f7e2-4c30-b69c-4c52d8bc85f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37322902-f1aa-4b9f-89dd-45885af7fd69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import components.models as models\n",
    "import components.models2 as models2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2e84cb-bc63-4773-aeff-74a4f5197ae9",
   "metadata": {},
   "source": [
    "## Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c81eac9b-f676-4652-a3f0-91f8e3d12466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ae = models.Autoencoder(5)\n",
    "# ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cee1be8c-5d73-4293-93ab-bba70995230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device\n",
    "\n",
    "# def train(autoencoder, data, epochs=20):\n",
    "#     opt = torch.optim.Adam(autoencoder.parameters())\n",
    "#     for epoch in range(epochs):\n",
    "#         for x, y in data:\n",
    "#             x = x.to(device) # GPU\n",
    "#             opt.zero_grad()\n",
    "#             x_hat = autoencoder(x)\n",
    "#             loss = ((x - x_hat)**2).sum()\n",
    "#             loss.backward()\n",
    "#             opt.step()\n",
    "#     return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4854a654-f2c6-4116-9c1f-e53431ccb6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder = train(autoencoder, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b450378-b186-4507-945c-2e5da1658529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent(autoencoder, data, num_batches=100):\n",
    "    for i, (x, y) in enumerate(data):\n",
    "        z = autoencoder.encoder(x.to(device))\n",
    "        z = z.to('cpu').detach().numpy()\n",
    "        plt.scatter(z[:, 0], z[:, 1], c=y, cmap='tab10')\n",
    "        if i > num_batches:\n",
    "            plt.colorbar()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee2b399d-e42e-47d1-b6d7-10af11b102b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_latent(autoencoder, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b6a89c39-ea95-4e78-b76f-61d473d0ef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstructed(autoencoder, r0=(-5, 10), r1=(-10, 5), n=12):\n",
    "    w = 28\n",
    "    img = np.zeros((n*w, n*w))\n",
    "    for i, y in enumerate(np.linspace(*r1, n)):\n",
    "        for j, x in enumerate(np.linspace(*r0, n)):\n",
    "            z = torch.Tensor([[x, y]]).to(device)\n",
    "            x_hat = autoencoder.decoder(z)\n",
    "            x_hat = x_hat.reshape(28, 28).to('cpu').detach().numpy()\n",
    "            img[(n-1-i)*w:(n-1-i+1)*w, j*w:(j+1)*w] = x_hat\n",
    "    plt.imshow(img, extent=[*r0, *r1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3a9919ff-281b-47cd-a76c-fb04eeebc8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_reconstructed(autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23684f83-beb5-4536-adff-54b0671090d0",
   "metadata": {},
   "source": [
    "## Variational Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00c79d0a-9ea7-4857-9f8a-9c97ffe6ded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae = models.VariationalAutoencoder(5)\n",
    "# vae\n",
    "\n",
    "input_size = 784\n",
    "hidden_sizes = [512,256,128,2]\n",
    "\n",
    "vae = models2.VariationalAutoencoder(input_size, hidden_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235ec7c0-ca0e-4621-a506-0377ae25c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(autoencoder, data, epochs=20):\n",
    "    opt = torch.optim.Adam(autoencoder.parameters())\n",
    "    total_loss = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in data:\n",
    "            x = x.to(device) # GPU\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            # forward pass\n",
    "            x_hat, mu, sigma = autoencoder(x)\n",
    "            \n",
    "            # calculate loss\n",
    "            # KL divergence of N(mu,sigma) from N(0,1)\n",
    "            kl = (sigma ** 2 + mu ** 2 - torch.log(sigma) - 1/2).sum() \n",
    "            \n",
    "            # difference of predictions\n",
    "            mse = ((x - x_hat)**2).sum()\n",
    "            loss = mse + kl\n",
    "            total_loss += loss\n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        print(f'epoch {epoch} loss:{total_loss /= len(self.data)}')\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccaeb450-f02d-46ef-b8c9-5af7ac19dd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "autoencoder = vae.to(device) # GPU\n",
    "\n",
    "data = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.MNIST('./data',\n",
    "               transform=torchvision.transforms.ToTensor(),\n",
    "               download=False),\n",
    "        batch_size=128,\n",
    "        shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61987128-aaeb-480d-8605-caa7a2331866",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "vae = train(vae, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752fd41f-7c1e-4e02-a6b0-eb3faeba9f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent(vae, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93c0467-680d-4f62-bd99-36ec7fb40eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reconstructed(vae, r0=(-3, 3), r1=(-3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df37bbc-43ec-4ebf-9922-0d9a3ccd89f9",
   "metadata": {},
   "source": [
    "## VAE source 2\n",
    "https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b338732b-d97b-4760-a1c5-0605ff136899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def kl_divergence(self, z, mu, std):\n",
    "#     # --------------------------\n",
    "#     # Monte carlo KL divergence\n",
    "#     # --------------------------\n",
    "#     # 1. define the first two probabilities (in this case Normal for both)\n",
    "#     p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "#     q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "#     # 2. get the probabilities from the equation\n",
    "#     log_qzx = q.log_prob(z)\n",
    "#     log_pz = p.log_prob(z)\n",
    "\n",
    "#     # kl\n",
    "#     kl = (log_qzx - log_pz)\n",
    "#     kl = kl.sum(-1)\n",
    "#     return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de69920b-dc5a-4987-b840-1201dcdc3b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gaussian_likelihood(self, x_hat, logscale, x):\n",
    "#     scale = torch.exp(logscale)\n",
    "#     mean = x_hat\n",
    "#     dist = torch.distributions.Normal(mean, scale)\n",
    "\n",
    "#     # measure prob of seeing image under p(x|z)\n",
    "#     log_pxz = dist.log_prob(x)\n",
    "#     return log_pxz.sum(dim=(1, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dbde0223-f631-4039-bd2b-7bdce5a54690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VAE(nn.Module):\n",
    "#     def __init__(self, enc_out_dim, latent_dim):\n",
    "#         super(VAE, self).__init__()\n",
    "        \n",
    "#         # self.encoder\n",
    "#         # self.decoder\n",
    "    \n",
    "#         # distribution parameters\n",
    "#         self.fc_mu = nn.Linear(enc_out_dim, latent_dim)\n",
    "#         self.fc_var = nn.Linear(enc_out_dim, latent_dim)\n",
    "        \n",
    "#         # for the gaussian likelihood\n",
    "#         self.log_scale = nn.Parameter(torch.Tensor([0.0]))\n",
    "        \n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         x, _ = batch\n",
    "\n",
    "#         # encode x to get the mu and variance parameters\n",
    "#         x_encoded = self.encoder(x)\n",
    "#         mu, log_var = self.fc_mu(x_encoded), self.fc_var(x_encoded)\n",
    "\n",
    "#         # sample z from q\n",
    "#         std = torch.exp(log_var / 2)\n",
    "#         q = torch.distributions.Normal(mu, std)\n",
    "#         z = q.rsample()\n",
    "\n",
    "#         # decoded\n",
    "#         x_hat = self.decoder(z)\n",
    "\n",
    "#         # reconstruction loss\n",
    "#         recon_loss = self.gaussian_likelihood(x_hat, self.log_scale, x)\n",
    "\n",
    "#         # kl\n",
    "#         kl = self.kl_divergence(z, mu, std)\n",
    "\n",
    "#         # elbo\n",
    "#         elbo = (kl - recon_loss)\n",
    "#         elbo = elbo.mean()\n",
    "\n",
    "#         self.log_dict({\n",
    "#             'elbo': elbo,\n",
    "#             'kl': kl.mean(),\n",
    "#             'recon_loss': recon_loss.mean(),\n",
    "#             'reconstruction': recon_loss.mean(),\n",
    "#             'kl': kl.mean(),\n",
    "#         })\n",
    "\n",
    "#         return elbo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89689110-68ea-493c-9012-365970793105",
   "metadata": {},
   "source": [
    "### VAE from paper\n",
    "Design of metalloproteins and\n",
    "novel protein folds using variational\n",
    "autoencoders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9be9ba9-4dc3-4419-aeeb-4902bf25f0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp = models2.MLP(sizes=[512, 256, 128, 16])\n",
    "# mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329fdde3-c492-4e1d-af42-f27252c4e9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# input_size=16\n",
    "# hidden_sizes=[128,256,512]\n",
    "# output_size=1024\n",
    "\n",
    "# dec2 = models2.Decoder(input_size, hidden_sizes, output_size)\n",
    "# dec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428c9672-7d83-4e5d-8d71-4e953932556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size=1024\n",
    "# hidden_sizes=[512,256,128]\n",
    "# output_size=16\n",
    "\n",
    "# enc2 = models2.Encoder(input_size, hidden_sizes, output_size)\n",
    "# enc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc25c50f-8f6a-40b6-8e8f-16e91eba48c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size=1024\n",
    "# hidden_sizes=[512,256,128,16]\n",
    "\n",
    "# vae = models2.VariationalAutoencoder(input_size, hidden_sizes)\n",
    "# vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362b03fc-4a07-4b1e-b81e-794bb4995581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0eda23-d4a2-494c-b927-1feb4607e161",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
